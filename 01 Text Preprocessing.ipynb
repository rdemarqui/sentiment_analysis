{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:16.748625Z",
     "start_time": "2023-08-18T20:43:15.984396Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:26.469060Z",
     "start_time": "2023-08-18T20:43:18.510608Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy.cli\n",
    "\n",
    "disable = ['tagger','parser','ner','entity_ruler','entity_linker','textcat']\n",
    "nlp = spacy.load('pt_core_news_lg', disable=disable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:42.703408Z",
     "start_time": "2023-08-18T20:43:38.989461Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:46.628544Z",
     "start_time": "2023-08-18T20:43:46.620563Z"
    }
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:49.352685Z",
     "start_time": "2023-08-18T20:43:48.677670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open dataframe\n",
    "columns = ['review_text', 'overall_rating', 'recommend_to_a_friend']\n",
    "df = pd.read_csv(os.path.join(path, 'data\\B2W-Reviews01.zip'),\n",
    "                 sep=',', compression='zip', usecols=columns)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:51.331233Z",
     "start_time": "2023-08-18T20:43:51.253867Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = df.query(\"overall_rating >= 4 or overall_rating <=2\").reset_index(drop=True).copy()\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:52.781515Z",
     "start_time": "2023-08-18T20:43:52.773315Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    for item in punctuations:\n",
    "        text = text.replace(item, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:53.380588Z",
     "start_time": "2023-08-18T20:43:53.366480Z"
    }
   },
   "outputs": [],
   "source": [
    "accentuation = {\n",
    "    \"á\": \"a\", \"ã\": \"a\", \"à\": \"a\",\"â\": \"a\",\n",
    "    \"é\": \"e\",\"ê\": \"e\",\n",
    "    \"í\": \"i\",\n",
    "    \"ó\": \"o\",\"õ\": \"o\", \"ô\":\"o\",\n",
    "    \"ú\": \"u\",\n",
    "    \"ç\": \"c\"\n",
    "    }\n",
    "def remove_accentuation(text):\n",
    "    for item in accentuation.items():\n",
    "        text = text.replace(str(item[0]), str(item[1]))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:53.795949Z",
     "start_time": "2023-08-18T20:43:53.771984Z"
    }
   },
   "outputs": [],
   "source": [
    "special_charaters = [\n",
    "    \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\n",
    "    \" a \",\" b \",\" c \",\" d \",\" e \",\" f \",\" g \",\" h \",\" i \",\" j \",\" k \",\" l \",\n",
    "    \" m \",\" n \",\" o \",\" p \",\" q \",\" r \",\" s \",\" t \",\" u \",\" v \",\" x \",\" z \",\n",
    "    \"r$\", \"$\"\n",
    "    ]\n",
    "def remove_special_characters(text):\n",
    "    for item in special_charaters:\n",
    "        text = text.replace(item, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:55.332160Z",
     "start_time": "2023-08-18T20:43:55.326090Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(df, text_field, lower=True, rem_punct=True, rem_accent=True, rem_spec_caract=True, rem_name=False):\n",
    "    df[text_field + \"_clean\"] = df[text_field].astype(str)\n",
    "    if lower: df[text_field + \"_clean\"] = df[text_field + \"_clean\"].str.lower()\n",
    "    if rem_punct: df[text_field + \"_clean\"] = df[text_field + \"_clean\"].apply(remove_punctuation)\n",
    "    if rem_accent: df[text_field + \"_clean\"] = df[text_field + \"_clean\"].apply(remove_accentuation)\n",
    "    if rem_spec_caract: df[text_field + \"_clean\"] = df[text_field + \"_clean\"].apply(remove_special_characters)\n",
    "    df[text_field + \"_clean\"] = df[text_field + \"_clean\"].replace(r'\\s+', ' ', regex=True) #remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:58.001750Z",
     "start_time": "2023-08-18T20:43:55.978474Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize_text(corpus, 'review_text')\n",
    "corpus.drop(columns=['review_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:43:59.253001Z",
     "start_time": "2023-08-18T20:43:59.229514Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "stops_spacy = nlp.Defaults.stop_words\n",
    "stop_words_join = set(stops_nltk) | stops_spacy\n",
    "print(f\"stop_nltk: {len(stops_nltk)}\\nstop_spacy: {len(stops_spacy)}\\nstop_join: {len(stop_words_join)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:44:16.621679Z",
     "start_time": "2023-08-18T20:44:16.607912Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = list(stops_nltk)\n",
    "stop_words = [remove_accentuation(word) for word in stop_words]\n",
    "stop_words = list(set(stop_words))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words_list = text.split()\n",
    "    words_list = [word for word in words_list if word not in stop_words]\n",
    "    text = ' '.join(words_list)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:44:24.081650Z",
     "start_time": "2023-08-18T20:44:20.334426Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[\"review_text_clean_stop\"] = corpus[\"review_text_clean\"].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T21:20:15.189827Z",
     "start_time": "2023-08-18T21:20:15.162924Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    lema_words = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lema_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T21:26:44.795835Z",
     "start_time": "2023-08-18T21:20:19.297273Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[\"review_text_clean_lema\"] = corpus[\"review_text_clean\"].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:35:23.424355Z",
     "start_time": "2023-08-18T19:35:23.319001Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:35:23.486580Z",
     "start_time": "2023-08-18T19:35:23.426208Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "def stematization(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:36:40.966129Z",
     "start_time": "2023-08-18T19:35:23.489610Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[\"review_text_clean_stem\"] = corpus[\"review_text_clean\"].apply(stematization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:37:23.131593Z",
     "start_time": "2023-08-18T19:37:23.112509Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapax Legomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:41:34.527909Z",
     "start_time": "2023-08-18T19:41:34.233159Z"
    }
   },
   "outputs": [],
   "source": [
    "full_text = ' '.join(corpus['review_text_clean'].tolist())\n",
    "words = full_text.split()\n",
    "vocabulary = set(words)\n",
    "print(f'Vocabulary size:{len(vocabulary)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:43:40.094423Z",
     "start_time": "2023-08-18T19:43:39.162773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Frequency distribution\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(words)\n",
    "word_freq = pd.DataFrame(fdist.most_common(len(vocabulary)), columns=['Word', 'Count'])\n",
    "word_freq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:59:53.447782Z",
     "start_time": "2023-08-18T19:59:53.427650Z"
    }
   },
   "outputs": [],
   "source": [
    "hapax_list = list(word_freq[word_freq['Count']<=1]['Word'])\n",
    "\n",
    "def hapax(text):\n",
    "    words_list = text.split()\n",
    "    words_list = [word for word in words_list if word not in hapax_list]\n",
    "    text = ' '.join(words_list)\n",
    "    \n",
    "print(f'Quantity hapax words {len(hapax_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T19:59:50.150001Z",
     "start_time": "2023-08-18T19:52:54.548184Z"
    }
   },
   "outputs": [],
   "source": [
    "#corpus[\"review_text_clean_hapax\"] = corpus[\"review_text_clean\"].apply(hapax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv('corpus.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
